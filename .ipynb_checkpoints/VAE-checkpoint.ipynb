{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba595575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import mlprepare as mlp \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import learning_curve,  GridSearchCV, cross_validate, KFold, cross_val_score\n",
    "from sklearn import metrics\n",
    "import scikitplot as skplt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c61cf",
   "metadata": {},
   "source": [
    "# VAE augmentation using PYTORCH from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bc691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(customLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction=\"sum\")\n",
    "    \n",
    "    def forward(self, x_recon, x, mu, logvar):\n",
    "        loss_MSE = self.mse_loss(x_recon, x)\n",
    "        loss_KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return loss_MSE + loss_KLD\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self,D_in=None,H=50,H2=12,latent_dim=3):\n",
    "\n",
    "        if D_in==None:\n",
    "            raise ValueError('You need to specify the Input shape.')\n",
    "        \n",
    "        #Encoder\n",
    "        super(Autoencoder,self).__init__()\n",
    "        self.linear1=nn.Linear(D_in,H)\n",
    "        self.lin_bn1 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear2=nn.Linear(H,H2)\n",
    "        self.lin_bn2 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear3=nn.Linear(H2,H2)\n",
    "        self.lin_bn3 = nn.BatchNorm1d(num_features=H2)\n",
    "        \n",
    "        # Latent vectors mu and sigma\n",
    "        self.fc1 = nn.Linear(H2, latent_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=latent_dim)\n",
    "        self.fc21 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Sampling vector\n",
    "        self.fc3 = nn.Linear(latent_dim, latent_dim)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(latent_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim, H2)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(H2)\n",
    "        \n",
    "        # Decoder\n",
    "        self.linear4=nn.Linear(H2,H2)\n",
    "        self.lin_bn4 = nn.BatchNorm1d(num_features=H2)\n",
    "        self.linear5=nn.Linear(H2,H)\n",
    "        self.lin_bn5 = nn.BatchNorm1d(num_features=H)\n",
    "        self.linear6=nn.Linear(H,D_in)\n",
    "        self.lin_bn6 = nn.BatchNorm1d(num_features=D_in)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        lin1 = self.relu(self.lin_bn1(self.linear1(x)))\n",
    "        lin2 = self.relu(self.lin_bn2(self.linear2(lin1)))\n",
    "        lin3 = self.relu(self.lin_bn3(self.linear3(lin2)))\n",
    "\n",
    "        fc1 = F.relu(self.bn1(self.fc1(lin3)))\n",
    "\n",
    "        r1 = self.fc21(fc1)\n",
    "        r2 = self.fc22(fc1)\n",
    "        \n",
    "        return r1, r2\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3)))\n",
    "\n",
    "        lin4 = self.relu(self.lin_bn4(self.linear4(fc4)))\n",
    "        lin5 = self.relu(self.lin_bn5(self.linear5(lin4)))\n",
    "        return self.lin_bn6(self.linear6(lin5))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "class AutoencoderModel:\n",
    "    def __init__(self,trainloader,testloader,device,D_in,H=50,H2=12,latent_dim=3):\n",
    "        self.trainloader=trainloader\n",
    "        self.testloader=testloader\n",
    "        self.device=device\n",
    "        self.D_in=D_in\n",
    "        self.H=H\n",
    "        self.H2=H2\n",
    "        self.latent_dim=latent_dim\n",
    "        self.model=Autoencoder(D_in, H, H2).to(self.device)\n",
    "        self.optimizer=optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.loss_mse = customLoss()\n",
    "    \n",
    "    def train_model(self,epoch, verbose, interval):\n",
    "        train_losses = []\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        for _, data in enumerate(self.trainloader):\n",
    "            data = data.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = self.model(data)\n",
    "            loss = self.loss_mse(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            self.optimizer.step()\n",
    "        if verbose:\n",
    "            if epoch % interval == 0:        \n",
    "                print('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "                    epoch, train_loss / len(self.trainloader.dataset)))\n",
    "                train_losses.append(train_loss / len(self.trainloader.dataset))\n",
    "\n",
    "    def test_model(self, epoch, verbose, interval):\n",
    "        test_losses = []\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0\n",
    "            for _, data in enumerate(self.testloader):\n",
    "                data = data.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_batch, mu, logvar = self.model(data)\n",
    "                loss = self.loss_mse(recon_batch, data, mu, logvar)\n",
    "                test_loss += loss.item()\n",
    "            if verbose:\n",
    "                if epoch % interval == 0:        \n",
    "                    print('====> Epoch: {} Average test loss: {:.4f}'.format(\n",
    "                        epoch, test_loss / len(self.testloader.dataset)))\n",
    "                test_losses.append(test_loss / len(self.testloader.dataset))\n",
    "\n",
    "    def fit(self, epochs, verbose=True, interval=200):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.train_model(epoch, verbose, interval)\n",
    "            self.test_model(epoch, verbose, interval)\n",
    "        return self\n",
    "\n",
    "    def predict(self, no_samples, target_class):\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(self.trainloader):\n",
    "                data = data.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                _, mu_, logvar_ = self.model(data)\n",
    "                if batch_idx==0:\n",
    "                    mu=mu_\n",
    "                    logvar=logvar_\n",
    "                else:\n",
    "                    mu=torch.cat((mu, mu_), dim=0)\n",
    "                    logvar=torch.cat((logvar, logvar_), dim=0)\n",
    "        sigma = torch.exp(logvar/2)\n",
    "        no_samples = no_samples\n",
    "        q = torch.distributions.Normal(mu.mean(axis=0), sigma.mean(axis=0))\n",
    "        z = q.rsample(sample_shape=torch.Size([no_samples]))\n",
    "        with torch.no_grad():\n",
    "            pred = self.model.decode(z).cpu().numpy()\n",
    "        df_fake = pd.DataFrame(pred)\n",
    "        df_fake['Class']=target_class\n",
    "        return df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04fd279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_18MinION= np.load(\"/home/alemsara/DirectRNA/datapsUAll/y_18MinION.npy\")\n",
    "X_18MinION_= pd.read_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_18MinION.pkl\")\n",
    "X_18MinION_ = X_18MinION_.fillna(0)\n",
    "X_18MinION_['Level'] = X_18MinION_['Level'].replace('ND',0)\n",
    "X_18MinION = X_18MinION_.drop(columns = ['Ref' , 'Pos','Base','ModStatus','Level','Cov1','Cov2','Min_cov'\n",
    "                                           , 'A1', 'C1', 'G1', 'T1', 'A2', 'C2', 'G2', 'T2'])\n",
    "X_18MinION = X_18MinION.fillna(0)\n",
    "\n",
    "y_18flongle= np.load(\"/home/alemsara/DirectRNA/datapsUAll/y_18flongle.npy\")\n",
    "X_18flongle_ = pd.read_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_18flongle.pkl\")\n",
    "X_18flongle_ = X_18flongle_.fillna(0)\n",
    "X_18flongle_['Level'] = X_18flongle_['Level'].replace('ND',0)\n",
    "X_18flongle = X_18flongle_.drop(columns = ['Ref' , 'Pos','Base','ModStatus','Level','Cov1','Cov2','Min_cov'\n",
    "                                           , 'A1', 'C1', 'G1', 'T1', 'A2', 'C2', 'G2', 'T2'])\n",
    "X_18flongle = X_18flongle.fillna(0)\n",
    "\n",
    "y_28flongle= np.load(\"/home/alemsara/DirectRNA/datapsUAll/y_28flongle.npy\")\n",
    "X_28flongle_ = pd.read_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_28flongle.pkl\")\n",
    "X_28flongle_ = X_28flongle_.fillna(0)\n",
    "X_28flongle_['Level'] = X_28flongle_['Level'].replace('ND',0)\n",
    "X_28flongle = X_28flongle_.drop(columns = ['Ref' , 'Pos','Base','ModStatus','Level','Cov1','Cov2','Min_cov'\n",
    "                                           , 'A1', 'C1', 'G1', 'T1', 'A2', 'C2', 'G2', 'T2'])\n",
    "X_28flongle = X_28flongle.fillna(0)\n",
    "\n",
    "y_28MinION= np.load(\"/home/alemsara/DirectRNA/datapsUAll/y_28MinIONv1.npy\")\n",
    "X_28MinION_= pd.read_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_28MinION.pkl\")\n",
    "X_28MinION_ = X_28MinION_.fillna(0)\n",
    "X_28MinION_['Level'] = X_28MinION_['Level'].replace('ND',0)\n",
    "X_28MinION = X_28MinION_.drop(columns = ['Ref' , 'Pos','Base','ModStatus','Level','Cov1','Cov2','Min_cov'\n",
    "                                           , 'A1', 'C1', 'G1', 'T1', 'A2', 'C2', 'G2', 'T2'])\n",
    "X_28MinION = X_28MinION.fillna(0)\n",
    "\n",
    "\n",
    "X_train= pd.concat([X_18MinION,X_28MinION])\n",
    "X_test = pd.concat([X_18flongle,X_28flongle])\n",
    "y_train = np.concatenate((y_18MinION,y_28MinION))\n",
    "y_test = np.concatenate((y_18flongle,y_28flongle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d14c9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_18MinION\n",
    "X_test = X_18flongle\n",
    "y_train = y_18MinION\n",
    "y_test = y_18flongle\n",
    "X_train_fraud = X_train.iloc[np.where(y_train==1)[0]]\n",
    "X_test_fraud = X_test.iloc[np.where(y_test==1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52e32ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_18MinION.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d79959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DataBuilder(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.x = dataset.values\n",
    "        self.x = torch.from_numpy(self.x).to(torch.float)\n",
    "        self.len=self.x.shape[0]\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "traindata_set=DataBuilder(X_train_fraud)\n",
    "testdata_set=DataBuilder(X_test_fraud)\n",
    "\n",
    "trainloader=DataLoader(dataset=traindata_set,batch_size=1024)\n",
    "testloader=DataLoader(dataset=testdata_set,batch_size=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b6ac24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "D_in = traindata_set.x.shape[1]\n",
    "H = 50\n",
    "H2 = 12\n",
    "autoenc_model = AutoencoderModel(trainloader, testloader, device, D_in, H, H2, latent_dim=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a2ee61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 200 Average training loss: 130.7960\n",
      "====> Epoch: 200 Average test loss: 1106.8752\n",
      "====> Epoch: 400 Average training loss: 124.6550\n",
      "====> Epoch: 400 Average test loss: 1120.8710\n",
      "====> Epoch: 600 Average training loss: 118.4296\n",
      "====> Epoch: 600 Average test loss: 1138.8858\n",
      "====> Epoch: 800 Average training loss: 112.9318\n",
      "====> Epoch: 800 Average test loss: 1132.6501\n",
      "====> Epoch: 1000 Average training loss: 107.2976\n",
      "====> Epoch: 1000 Average test loss: 1158.1762\n",
      "====> Epoch: 1200 Average training loss: 102.4158\n",
      "====> Epoch: 1200 Average test loss: 1162.9567\n",
      "====> Epoch: 1400 Average training loss: 96.9689\n",
      "====> Epoch: 1400 Average test loss: 1172.5975\n",
      "====> Epoch: 1600 Average training loss: 92.2373\n",
      "====> Epoch: 1600 Average test loss: 1183.2899\n",
      "====> Epoch: 1800 Average training loss: 87.7009\n",
      "====> Epoch: 1800 Average test loss: 1190.5964\n",
      "====> Epoch: 2000 Average training loss: 83.8914\n",
      "====> Epoch: 2000 Average test loss: 1209.1267\n",
      "====> Epoch: 2200 Average training loss: 79.6187\n",
      "====> Epoch: 2200 Average test loss: 1212.4787\n",
      "====> Epoch: 2400 Average training loss: 75.6858\n",
      "====> Epoch: 2400 Average test loss: 1209.7453\n",
      "====> Epoch: 2600 Average training loss: 71.6542\n",
      "====> Epoch: 2600 Average test loss: 1231.9447\n",
      "====> Epoch: 2800 Average training loss: 68.1677\n",
      "====> Epoch: 2800 Average test loss: 1231.9768\n",
      "====> Epoch: 3000 Average training loss: 65.1729\n",
      "====> Epoch: 3000 Average test loss: 1230.4443\n",
      "====> Epoch: 3200 Average training loss: 61.9459\n",
      "====> Epoch: 3200 Average test loss: 1246.1362\n",
      "====> Epoch: 3400 Average training loss: 59.1987\n",
      "====> Epoch: 3400 Average test loss: 1254.9318\n",
      "====> Epoch: 3600 Average training loss: 56.0476\n",
      "====> Epoch: 3600 Average test loss: 1268.3891\n",
      "====> Epoch: 3800 Average training loss: 53.4330\n",
      "====> Epoch: 3800 Average test loss: 1273.7485\n",
      "====> Epoch: 4000 Average training loss: 51.1307\n",
      "====> Epoch: 4000 Average test loss: 1271.5430\n",
      "====> Epoch: 4200 Average training loss: 48.7564\n",
      "====> Epoch: 4200 Average test loss: 1283.7770\n",
      "====> Epoch: 4400 Average training loss: 46.6712\n",
      "====> Epoch: 4400 Average test loss: 1287.1784\n",
      "====> Epoch: 4600 Average training loss: 44.3736\n",
      "====> Epoch: 4600 Average test loss: 1294.5382\n",
      "====> Epoch: 4800 Average training loss: 42.2892\n",
      "====> Epoch: 4800 Average test loss: 1305.0246\n",
      "====> Epoch: 5000 Average training loss: 40.5302\n",
      "====> Epoch: 5000 Average test loss: 1296.7368\n",
      "====> Epoch: 5200 Average training loss: 38.6260\n",
      "====> Epoch: 5200 Average test loss: 1306.3940\n",
      "====> Epoch: 5400 Average training loss: 36.3749\n",
      "====> Epoch: 5400 Average test loss: 1316.0650\n",
      "====> Epoch: 5600 Average training loss: 34.7193\n",
      "====> Epoch: 5600 Average test loss: 1309.1646\n",
      "====> Epoch: 5800 Average training loss: 32.6385\n",
      "====> Epoch: 5800 Average test loss: 1314.3424\n",
      "====> Epoch: 6000 Average training loss: 31.2599\n",
      "====> Epoch: 6000 Average test loss: 1325.1703\n",
      "====> Epoch: 6200 Average training loss: 29.7724\n",
      "====> Epoch: 6200 Average test loss: 1316.6469\n",
      "====> Epoch: 6400 Average training loss: 27.9096\n",
      "====> Epoch: 6400 Average test loss: 1332.1784\n",
      "====> Epoch: 6600 Average training loss: 26.9265\n",
      "====> Epoch: 6600 Average test loss: 1325.5666\n",
      "====> Epoch: 6800 Average training loss: 24.9848\n",
      "====> Epoch: 6800 Average test loss: 1326.4156\n",
      "====> Epoch: 7000 Average training loss: 24.2969\n",
      "====> Epoch: 7000 Average test loss: 1336.4367\n",
      "====> Epoch: 7200 Average training loss: 22.7225\n",
      "====> Epoch: 7200 Average test loss: 1335.9040\n",
      "====> Epoch: 7400 Average training loss: 21.4489\n",
      "====> Epoch: 7400 Average test loss: 1335.9593\n",
      "====> Epoch: 7600 Average training loss: 20.2699\n",
      "====> Epoch: 7600 Average test loss: 1337.7169\n",
      "====> Epoch: 7800 Average training loss: 19.0850\n",
      "====> Epoch: 7800 Average test loss: 1349.1489\n",
      "====> Epoch: 8000 Average training loss: 18.0899\n",
      "====> Epoch: 8000 Average test loss: 1345.6685\n",
      "====> Epoch: 8200 Average training loss: 16.8970\n",
      "====> Epoch: 8200 Average test loss: 1347.4778\n",
      "====> Epoch: 8400 Average training loss: 17.1353\n",
      "====> Epoch: 8400 Average test loss: 1345.7211\n",
      "====> Epoch: 8600 Average training loss: 15.5347\n",
      "====> Epoch: 8600 Average test loss: 1348.8555\n",
      "====> Epoch: 8800 Average training loss: 14.2610\n",
      "====> Epoch: 8800 Average test loss: 1352.9267\n",
      "====> Epoch: 9000 Average training loss: 14.0531\n",
      "====> Epoch: 9000 Average test loss: 1361.9161\n",
      "====> Epoch: 9200 Average training loss: 13.4684\n",
      "====> Epoch: 9200 Average test loss: 1373.4180\n",
      "====> Epoch: 9400 Average training loss: 12.6096\n",
      "====> Epoch: 9400 Average test loss: 1355.7201\n",
      "====> Epoch: 9600 Average training loss: 12.3869\n",
      "====> Epoch: 9600 Average test loss: 1361.3187\n",
      "====> Epoch: 9800 Average training loss: 11.5964\n",
      "====> Epoch: 9800 Average test loss: 1371.7018\n",
      "====> Epoch: 10000 Average training loss: 11.4282\n",
      "====> Epoch: 10000 Average test loss: 1375.6621\n",
      "====> Epoch: 10200 Average training loss: 11.2073\n",
      "====> Epoch: 10200 Average test loss: 1379.3624\n",
      "====> Epoch: 10400 Average training loss: 10.1099\n",
      "====> Epoch: 10400 Average test loss: 1372.3874\n",
      "====> Epoch: 10600 Average training loss: 10.2368\n",
      "====> Epoch: 10600 Average test loss: 1375.7724\n",
      "====> Epoch: 10800 Average training loss: 9.9938\n",
      "====> Epoch: 10800 Average test loss: 1397.7987\n",
      "====> Epoch: 11000 Average training loss: 9.1421\n",
      "====> Epoch: 11000 Average test loss: 1392.3810\n",
      "====> Epoch: 11200 Average training loss: 9.3972\n",
      "====> Epoch: 11200 Average test loss: 1389.9672\n",
      "====> Epoch: 11400 Average training loss: 8.8517\n",
      "====> Epoch: 11400 Average test loss: 1405.5915\n",
      "====> Epoch: 11600 Average training loss: 9.2734\n",
      "====> Epoch: 11600 Average test loss: 1401.1800\n",
      "====> Epoch: 11800 Average training loss: 8.6435\n",
      "====> Epoch: 11800 Average test loss: 1398.8182\n",
      "====> Epoch: 12000 Average training loss: 9.1522\n",
      "====> Epoch: 12000 Average test loss: 1388.3228\n"
     ]
    }
   ],
   "source": [
    "autoenc_model_fit = autoenc_model.fit(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f7d1b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    354\n",
       "1     42\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4449daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake = autoenc_model_fit.predict(no_samples=42,target_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9694d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fake.columns.values[:-1] = X_train.columns\n",
    "df_fake['Class'] = np.round(df_fake['Class']).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eabcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augmented = X_train.append(df_fake.iloc[:,:-1]).reset_index(drop=True)\n",
    "X_train_augmented.head()\n",
    "X_train_augmented=X_train_augmented.fillna(0)\n",
    "y_train_augmented = np.append(y_train,df_fake.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a1ee19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_augmented.to_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_train_augmented.pkl\")\n",
    "np.save(\"/home/alemsara/DirectRNA/datapsUAll/y_train_augmented\", y_train_augmented)\n",
    "# X_train.to_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_train.pkl\")\n",
    "# np.save(\"/home/alemsara/DirectRNA/datapsUAll/y_train\", y_train)\n",
    "# X_test.to_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_test.pkl\")\n",
    "# np.save(\"/home/alemsara/DirectRNA/datapsUAll/y_test\", y_test)\n",
    "# X_test_.to_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_test_.pkl\")\n",
    "# X_train_.to_pickle(\"/home/alemsara/DirectRNA/datapsUAll/X_train_.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3267e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DirectRNA",
   "language": "python",
   "name": "directrna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
